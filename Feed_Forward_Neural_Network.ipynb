{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward Neural Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCfq28YjTXUp1l/rphpTdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathameshbhalekar/Neural-Network-From-Scratch/blob/master/Feed_Forward_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvU7AiMWbNMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(self,layers,activation_functions,loss_function='ce'):\n",
        "        self.layers=layers\n",
        "        self.layer_count=len(layers)\n",
        "        self.activation_functions=activation_functions\n",
        "        self.loss_function=loss_function\n",
        "        assert self.layer_count==len(self.activation_functions)+1\n",
        "        self.weights=[]\n",
        "        for i in range(1,self.layer_count):\n",
        "          self.weights.append(np.random.randn(self.layers[i],self.layers[i-1]))\n",
        "        self.biases = [np.zeros((x, 1)) for x in self.layers[1:]]\n",
        "        \n",
        "    def forwardPass(self,X):\n",
        "      self.inputs=[]\n",
        "      self.outputs=[]\n",
        "      W=self.weights\n",
        "      b=self.biases\n",
        "\n",
        "\n",
        "      for layer in range(0,self.layer_count-1):\n",
        "        if layer==0:\n",
        "          self.inputs.append(np.matmul(W[layer],X.T)+self.biases[layer])\n",
        "        else:\n",
        "          self.inputs.append(np .matmul(W[layer],self.outputs[layer-1])+self.biases[layer])\n",
        "\n",
        "        if self.activation_functions[layer]=='relu':\n",
        "          self.outputs.append(self.relu(self.inputs[layer]))\n",
        "        elif self.activation_functions[layer]=='stable_softmax':\n",
        "          self.outputs.append(self.stable_softmax(self.inputs[layer]))\n",
        "        elif self.activation_functions[layer]=='sigmoid':\n",
        "          self.outputs.append(self.sigmoid(self.inputs[layer]))\n",
        "      return self.outputs[-1]\n",
        "\n",
        "    def backwardPropogate(self,X,Y,prediction):\n",
        "      grad_layers=[0 for _ in range(self.layer_count)]\n",
        "      grad_weights=[0 for _ in range(len(self.weights))]\n",
        "      grad_biases=[0 for _ in range(len(self.biases))]\n",
        "      samples=X.shape[0]\n",
        "      for layer in reversed(range(self.layer_count-1)):\n",
        "        if layer==self.layer_count-2:\n",
        "          if self.loss_function=='ce':\n",
        "            grad_layers[layer]=prediction-Y.T\n",
        "        else:\n",
        "          if self.activation_functions[layer]=='relu':\n",
        "            grad_layers[layer]=(np.matmul(self.weights[layer+1].T,grad_layers[layer+1]))*self.relu(grad_layers[layer],derivative=True)\n",
        "\n",
        "        \n",
        "        if layer!=0:\n",
        "          grad_weights[layer]=(1/samples) * np.matmul(grad_layers[layer],self.outputs[layer-1].T)\n",
        "        else:\n",
        "          grad_weights[layer]=(1/samples) * np.matmul(grad_layers[layer],X)\n",
        "        grad_biases[layer]=(1/samples) * np.sum(grad_layers[layer],axis=1,keepdims=True)\n",
        "      return (grad_layers,grad_weights,grad_biases)\n",
        "    \n",
        "    def train(self,X,Y,eta=0.01,batch_size=30,max_epochs=100,optimizer='gd',momentum=0.8):\n",
        "      training_loss=[]\n",
        "      t=0\n",
        "      v_b=0\n",
        "      v_w=0\n",
        "      m_b=0\n",
        "      m_w=0\n",
        "      prev_grad_weights=np.multiply(self.weights,0)\n",
        "      prev_grad_biases=np.multiply(self.biases,0)\n",
        "      for epoch in range (max_epochs):\n",
        "        step=0\n",
        "        batch_loss=0\n",
        "        for num in range(0,X.shape[0],batch_size):\n",
        "          batch_X=X[num:num+batch_size]\n",
        "          batch_Y=Y[num:num+batch_size]\n",
        "          predictions=self.forwardPass(batch_X)\n",
        "          (grad_layers, grad_weights,grad_biases) = self.backwardPropogate(batch_X,batch_Y, predictions)\n",
        "          if optimizer=='gd':\n",
        "            update_w = np.multiply(eta,grad_weights)\n",
        "            update_b = np.multiply(eta,grad_biases)\n",
        "            \n",
        "          if optimizer=='momentum':\n",
        "            update_w = np.multiply(eta,grad_weights)+np.multiply(momentum,prev_grad_weights)\n",
        "            update_b = np.multiply(eta,grad_biases)+np.multiply(momentum,prev_grad_biases)\n",
        "            prev_grad_weights=update_w\n",
        "            prev_grad_biases=update_b\n",
        "          if optimizer=='adagrad':\n",
        "            e=1e-8\n",
        "            v_w=v_w+np.power(grad_weights,2)\n",
        "            v_b=v_b+np.power(grad_biases,2)\n",
        "            mul_w=1/np.power(v_w+e,1/2)\n",
        "            mul_b=1/np.power(v_b+e,1/2)\n",
        "            update_w=np.multiply(grad_weights,mul_w)*eta\n",
        "            update_b=np.multiply(grad_biases,mul_b)*eta\n",
        "            \n",
        "          if optimizer=='rms_prop':\n",
        "            e=1e-8\n",
        "            beta=0.95\n",
        "            v_w=v_w*beta+(1-beta)*np.power(grad_weights,2)\n",
        "            v_b=v_b*beta+(1-beta)*np.power(grad_biases,2)\n",
        "            mul_w=1/np.power(v_w+e,1/2)\n",
        "            mul_b=1/np.power(v_b+e,1/2)\n",
        "            update_w=np.multiply(grad_weights,mul_w)*eta\n",
        "            update_b=np.multiply(grad_biases,mul_b)*eta\n",
        "          if optimizer=='adam':\n",
        "            e=1e-8\n",
        "            beta1=0.95\n",
        "            beta2=0.99\n",
        "            \n",
        "            m_w=np.multiply(m_w,beta1)+np.multiply((1-beta1),grad_weights)\n",
        "            m_b=np.multiply(m_b,beta1)+np.multiply((1-beta1),grad_biases)\n",
        "\n",
        "            v_w=v_w*beta2+(1-beta2)*np.power(grad_weights,2)\n",
        "            v_b=v_b*beta2+(1-beta2)*np.power(grad_biases,2)\n",
        "\n",
        "            m_w=m_w/(1-np.power(beta1,t+1))\n",
        "            m_b=m_b/(1-np.power(beta1,t+1))\n",
        "\n",
        "            v_w=v_w/(1-np.power(beta2,t+1))\n",
        "            v_b=v_b/(1-np.power(beta2,t+1))\n",
        "\n",
        "            mul_w=1/np.power(v_w+e,1/2)\n",
        "            mul_b=1/np.power(v_b+e,1/2)\n",
        "\n",
        "            update_w=np.multiply(mul_w,m_w)*eta\n",
        "            update_b=np.multiply(mul_b,m_b)*eta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          self.weights=self.weights-update_w\n",
        "          self.biases=self.biases-update_b\n",
        "          t=t+1\n",
        "          if self.loss_function == 'ce':           \n",
        "              batch_loss += log_loss(batch_Y.T,predictions)\n",
        "        training_loss.append(batch_loss)\n",
        "        predictions_train=self.forwardPass(X)\n",
        "        (acc_train, correct_train,total_train) = self.evaluate(predictions_train.T, Y)\n",
        "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "        print('Epoch {0} Training Loss {1}'.format(epoch, training_loss[epoch]))\n",
        "        print('Epoch {0} : Training Accuracy : {1}%'.format(epoch, round(acc_train , 2)))\n",
        "            \n",
        "\n",
        "    def relu(self, x, derivative = False):\n",
        "        if derivative == False:\n",
        "            return np.maximum(0, x, x)\n",
        "        else:\n",
        "            return np.greater(x,0).astype(int)\n",
        "    \n",
        "    def stable_softmax(self, x, derivative = False):\n",
        "        if derivative == False:\n",
        "            out = np.zeros(x.shape)\n",
        "            for i in range(0, x.shape[1]):\n",
        "                exps = np.exp(x[:, i] - np.max(x[:, i]))\n",
        "                out[:, i] = exps / np.sum( exps)\n",
        "                \n",
        "            return out\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        return\n",
        "    def softmax(self, x, derivative = False):\n",
        "        if derivative == False:\n",
        "            \n",
        "            out = np.zeros(x.shape)\n",
        "            \n",
        "            for i in range(0, x.shape[1]):\n",
        "                exps = np.exp(x[:, i])\n",
        "                out[:, i] = exps / np.sum( exps)\n",
        "                \n",
        "            return out\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        return\n",
        "    def evaluate(self, predictions, y):\n",
        "        accuracyMatrix = np.argmax(predictions, axis = 1).reshape(y.shape[0], 1) == np.argmax(y, axis = 1).reshape(y.shape[0], 1)\n",
        "        accuracyList = accuracyMatrix.tolist()\n",
        "        correct = accuracyList.count([True])\n",
        "        total = len(accuracyList)\n",
        "        acc = correct/total\n",
        "        acc = round(acc*100, 2)\n",
        "        return (acc,correct,total)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_YHGJga6s6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import datasets\n",
        "(train_featureset,train_labels),(test_featureset,test_labels)=datasets.fashion_mnist.load_data()\n",
        "X=[]\n",
        "train_featureset=train_featureset/255.0\n",
        "for i in range(train_featureset.shape[0]):\n",
        "    X.append(train_featureset[i].reshape(28*28))\n",
        "X_test=[]\n",
        "test_featureset=test_featureset/255.0\n",
        "for i in range(test_featureset.shape[0]):\n",
        "    X_test.append(test_featureset[i].reshape(28*28))\n",
        "Y=np.zeros((train_labels.shape[0],10))\n",
        "for i in range(train_labels.shape[0]):\n",
        "    Y[i][train_labels[i]]=1\n",
        "Y_test=np.zeros((test_labels.shape[0],10))\n",
        "for i in range(test_labels.shape[0]):\n",
        "    Y_test[i][test_labels[i]]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44qASm-egZwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e22e232-2281-4e37-b1c8-933e58b7b0e6"
      },
      "source": [
        "model=FeedForwardNeuralNetwork([28*28,64,128,10],['relu','relu','stable_softmax'])\n",
        "model.train(np.array(X),np.array(Y),max_epochs=20,optimizer='adam',momentum=0.2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 0 Training Loss 148388.40091072003\n",
            "Epoch 0 : Training Accuracy : 7.9%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1 Training Loss 148375.38743050324\n",
            "Epoch 1 : Training Accuracy : 7.9%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 2 Training Loss 148349.79126574486\n",
            "Epoch 2 : Training Accuracy : 7.9%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 3 Training Loss 148335.9398446314\n",
            "Epoch 3 : Training Accuracy : 7.9%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 4 Training Loss 148197.47715493402\n",
            "Epoch 4 : Training Accuracy : 7.95%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 5 Training Loss 147954.2880492598\n",
            "Epoch 5 : Training Accuracy : 8.03%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 6 Training Loss 149198.45190135142\n",
            "Epoch 6 : Training Accuracy : 9.07%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 7 Training Loss 131059.98428081996\n",
            "Epoch 7 : Training Accuracy : 66.15%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 8 Training Loss 55760.42599720893\n",
            "Epoch 8 : Training Accuracy : 72.06%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 9 Training Loss 46582.72904607287\n",
            "Epoch 9 : Training Accuracy : 73.56%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 10 Training Loss 41608.15401781919\n",
            "Epoch 10 : Training Accuracy : 70.63%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 11 Training Loss 39429.53783049703\n",
            "Epoch 11 : Training Accuracy : 74.02%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 12 Training Loss 37739.81984071672\n",
            "Epoch 12 : Training Accuracy : 72.07%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 13 Training Loss 37623.179303309036\n",
            "Epoch 13 : Training Accuracy : 71.94%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 14 Training Loss 36484.06498098421\n",
            "Epoch 14 : Training Accuracy : 71.38%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 15 Training Loss 36329.04051641548\n",
            "Epoch 15 : Training Accuracy : 72.08%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 16 Training Loss 35515.52476100275\n",
            "Epoch 16 : Training Accuracy : 73.38%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 17 Training Loss 35309.96661623443\n",
            "Epoch 17 : Training Accuracy : 73.33%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 18 Training Loss 35368.08813318132\n",
            "Epoch 18 : Training Accuracy : 74.01%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 19 Training Loss 35474.65108530835\n",
            "Epoch 19 : Training Accuracy : 72.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHQt_WWI6_9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def predict(pos):\n",
        "  v=model.forwardPass(np.array([X_test[pos]]))\n",
        "  plt.imshow(X_test[pos].reshape((28,28)))\n",
        "  v=v.T[0]\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "  l=max(v)\n",
        "  c=0\n",
        "  for i in v:\n",
        "      if i==l:\n",
        "          print(class_names[c])\n",
        "      c=c+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYzgo_nhKSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "ea59f452-ca10-45ea-cc38-b2fa02d52ba9"
      },
      "source": [
        "predict(724)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shirt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}