{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks From Scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOWz09As/xc3tJKfYdvP2o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathameshbhalekar/Neural-Network-From-Scratch/blob/master/Neural_Networks_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvU7AiMWbNMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(self,layers,activation_functions,loss_function='ce'):\n",
        "        self.layers=layers\n",
        "        self.layer_count=len(layers)\n",
        "        self.activation_functions=activation_functions\n",
        "        self.loss_function=loss_function\n",
        "        assert self.layer_count==len(self.activation_functions)+1\n",
        "        self.weights=[]\n",
        "        for i in range(1,self.layer_count):\n",
        "          self.weights.append(np.random.randn(self.layers[i],self.layers[i-1]))\n",
        "        self.biases = [np.zeros((x, 1)) for x in self.layers[1:]]\n",
        "        \n",
        "    def forwardPass(self,X):\n",
        "      self.inputs=[]\n",
        "      self.outputs=[]\n",
        "      W=self.weights\n",
        "      b=self.biases\n",
        "\n",
        "\n",
        "      for layer in range(0,self.layer_count-1):\n",
        "        if layer==0:\n",
        "          self.inputs.append(np.matmul(W[layer],X.T)+self.biases[layer])\n",
        "        else:\n",
        "          self.inputs.append(np .matmul(W[layer],self.outputs[layer-1])+self.biases[layer])\n",
        "\n",
        "        if self.activation_functions[layer]=='relu':\n",
        "          self.outputs.append(self.relu(self.inputs[layer]))\n",
        "        elif self.activation_functions[layer]=='stable_softmax':\n",
        "          self.outputs.append(self.stable_softmax(self.inputs[layer]))\n",
        "        elif self.activation_functions[layer]=='sigmoid':\n",
        "          self.outputs.append(self.sigmoid(self.inputs[layer]))\n",
        "      return self.outputs[-1]\n",
        "\n",
        "    def backwardPropogate(self,X,Y,prediction):\n",
        "      grad_layers=[0 for _ in range(self.layer_count)]\n",
        "      grad_weights=[0 for _ in range(len(self.weights))]\n",
        "      grad_biases=[0 for _ in range(len(self.biases))]\n",
        "      samples=X.shape[0]\n",
        "      for layer in reversed(range(self.layer_count-1)):\n",
        "        if layer==self.layer_count-2:\n",
        "          if self.loss_function=='ce':\n",
        "            grad_layers[layer]=prediction-Y.T\n",
        "        else:\n",
        "          if self.activation_functions[layer]=='relu':\n",
        "            grad_layers[layer]=(np.matmul(self.weights[layer+1].T,grad_layers[layer+1]))*self.relu(grad_layers[layer],derivative=True)\n",
        "\n",
        "        \n",
        "        if layer!=0:\n",
        "          grad_weights[layer]=(1/samples) * np.matmul(grad_layers[layer],self.outputs[layer-1].T)\n",
        "        else:\n",
        "          grad_weights[layer]=(1/samples) * np.matmul(grad_layers[layer],X)\n",
        "        grad_biases[layer]=(1/samples) * np.sum(grad_layers[layer],axis=1,keepdims=True)\n",
        "      return (grad_layers,grad_weights,grad_biases)\n",
        "    \n",
        "    def train(self,X,Y,eta=0.01,batch_size=30,max_epochs=100,optimizer='gd',momentum=0.8):\n",
        "      training_loss=[]\n",
        "      t=0\n",
        "      v_b=0\n",
        "      v_w=0\n",
        "      m_b=0\n",
        "      m_w=0\n",
        "      prev_grad_weights=np.multiply(self.weights,0)\n",
        "      prev_grad_biases=np.multiply(self.biases,0)\n",
        "      for epoch in range (max_epochs):\n",
        "        step=0\n",
        "        batch_loss=0\n",
        "        for num in range(0,X.shape[0],batch_size):\n",
        "          batch_X=X[num:num+batch_size]\n",
        "          batch_Y=Y[num:num+batch_size]\n",
        "          predictions=self.forwardPass(batch_X)\n",
        "          (grad_layers, grad_weights,grad_biases) = self.backwardPropogate(batch_X,batch_Y, predictions)\n",
        "          if optimizer=='gd':\n",
        "            update_w = np.multiply(eta,grad_weights)\n",
        "            update_b = np.multiply(eta,grad_biases)\n",
        "            \n",
        "          if optimizer=='momentum':\n",
        "            update_w = np.multiply(eta,grad_weights)+np.multiply(momentum,prev_grad_weights)\n",
        "            update_b = np.multiply(eta,grad_biases)+np.multiply(momentum,prev_grad_biases)\n",
        "            prev_grad_weights=update_w\n",
        "            prev_grad_biases=update_b\n",
        "          if optimizer=='adagrad':\n",
        "            e=1e-8\n",
        "            v_w=v_w+np.power(grad_weights,2)\n",
        "            v_b=v_b+np.power(grad_biases,2)\n",
        "            mul_w=1/np.power(v_w+e,1/2)\n",
        "            mul_b=1/np.power(v_b+e,1/2)\n",
        "            update_w=np.multiply(grad_weights,mul_w)*eta\n",
        "            update_b=np.multiply(grad_biases,mul_b)*eta\n",
        "            \n",
        "          if optimizer=='rms_prop':\n",
        "            e=1e-8\n",
        "            beta=0.95\n",
        "            v_w=v_w*beta+(1-beta)*np.power(grad_weights,2)\n",
        "            v_b=v_b*beta+(1-beta)*np.power(grad_biases,2)\n",
        "            mul_w=1/np.power(v_w+e,1/2)\n",
        "            mul_b=1/np.power(v_b+e,1/2)\n",
        "            update_w=np.multiply(grad_weights,mul_w)*eta\n",
        "            update_b=np.multiply(grad_biases,mul_b)*eta\n",
        "          if optimizer=='adam':\n",
        "            e=1e-8\n",
        "            beta1=0.95\n",
        "            beta2=0.99\n",
        "            \n",
        "            m_w=np.multiply(m_w,beta1)+np.multiply((1-beta1),grad_weights)\n",
        "            m_b=np.multiply(m_b,beta1)+np.multiply((1-beta1),grad_biases)\n",
        "\n",
        "            v_w=v_w*beta2+(1-beta2)*np.power(grad_weights,2)\n",
        "            v_b=v_b*beta2+(1-beta2)*np.power(grad_biases,2)\n",
        "\n",
        "            m_w=m_w/(1-np.power(beta1,t+1))\n",
        "            m_b=m_b/(1-np.power(beta1,t+1))\n",
        "\n",
        "            v_w=v_w/(1-np.power(beta2,t+1))\n",
        "            v_b=v_b/(1-np.power(beta2,t+1))\n",
        "\n",
        "            mul_w=1/np.power(v_w+e,1/2)\n",
        "            mul_b=1/np.power(v_b+e,1/2)\n",
        "\n",
        "            update_w=np.multiply(mul_w,m_w)*eta\n",
        "            update_b=np.multiply(mul_b,m_b)*eta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          self.weights=self.weights-update_w\n",
        "          self.biases=self.biases-update_b\n",
        "          t=t+1\n",
        "          if self.loss_function == 'ce':           \n",
        "              batch_loss += log_loss(batch_Y.T,predictions)\n",
        "        training_loss.append(batch_loss)\n",
        "        predictions_train=self.forwardPass(X)\n",
        "        (acc_train, correct_train,total_train) = self.evaluate(predictions_train.T, Y)\n",
        "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "        print('Epoch {0} Training Loss {1}'.format(epoch, training_loss[epoch]))\n",
        "        print('Epoch {0} : Training Accuracy : {1}%'.format(epoch, round(acc_train , 2)))\n",
        "            \n",
        "\n",
        "    def relu(self, x, derivative = False):\n",
        "        if derivative == False:\n",
        "            return np.maximum(0, x, x)\n",
        "        else:\n",
        "            return np.greater(x,0).astype(int)\n",
        "    \n",
        "    def stable_softmax(self, x, derivative = False):\n",
        "        if derivative == False:\n",
        "            out = np.zeros(x.shape)\n",
        "            for i in range(0, x.shape[1]):\n",
        "                exps = np.exp(x[:, i] - np.max(x[:, i]))\n",
        "                out[:, i] = exps / np.sum( exps)\n",
        "                \n",
        "            return out\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        return\n",
        "    def softmax(self, x, derivative = False):\n",
        "        if derivative == False:\n",
        "            \n",
        "            out = np.zeros(x.shape)\n",
        "            \n",
        "            for i in range(0, x.shape[1]):\n",
        "                exps = np.exp(x[:, i])\n",
        "                out[:, i] = exps / np.sum( exps)\n",
        "                \n",
        "            return out\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        return\n",
        "    def evaluate(self, predictions, y):\n",
        "        accuracyMatrix = np.argmax(predictions, axis = 1).reshape(y.shape[0], 1) == np.argmax(y, axis = 1).reshape(y.shape[0], 1)\n",
        "        accuracyList = accuracyMatrix.tolist()\n",
        "        correct = accuracyList.count([True])\n",
        "        total = len(accuracyList)\n",
        "        acc = correct/total\n",
        "        acc = round(acc*100, 2)\n",
        "        return (acc,correct,total)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_YHGJga6s6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import datasets\n",
        "(train_featureset,train_labels),(test_featureset,test_labels)=datasets.fashion_mnist.load_data()\n",
        "X=[]\n",
        "train_featureset=train_featureset/255.0\n",
        "for i in range(train_featureset.shape[0]):\n",
        "    X.append(train_featureset[i].reshape(28*28))\n",
        "X_test=[]\n",
        "test_featureset=test_featureset/255.0\n",
        "for i in range(test_featureset.shape[0]):\n",
        "    X_test.append(test_featureset[i].reshape(28*28))\n",
        "Y=np.zeros((train_labels.shape[0],10))\n",
        "for i in range(train_labels.shape[0]):\n",
        "    Y[i][train_labels[i]]=1\n",
        "Y_test=np.zeros((test_labels.shape[0],10))\n",
        "for i in range(test_labels.shape[0]):\n",
        "    Y_test[i][test_labels[i]]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44qASm-egZwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52b71c54-8c1e-470e-bdcf-6c4c124c48a9"
      },
      "source": [
        "model=FeedForwardNeuralNetwork([28*28,64,128,10],['relu','relu','stable_softmax'])\n",
        "model.train(np.array(X),np.array(Y),max_epochs=20,optimizer='adam',momentum=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 0 Training Loss 116825.87744293967\n",
            "Epoch 0 : Training Accuracy : 7.32%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 1 Training Loss 116773.02519225438\n",
            "Epoch 1 : Training Accuracy : 7.32%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 2 Training Loss 116768.74740310847\n",
            "Epoch 2 : Training Accuracy : 7.33%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 3 Training Loss 116773.8339389714\n",
            "Epoch 3 : Training Accuracy : 7.33%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 4 Training Loss 116850.24818642353\n",
            "Epoch 4 : Training Accuracy : 7.34%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 5 Training Loss 116858.63581525598\n",
            "Epoch 5 : Training Accuracy : 7.34%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 6 Training Loss 117024.15731665652\n",
            "Epoch 6 : Training Accuracy : 7.8%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 7 Training Loss 120539.38251060656\n",
            "Epoch 7 : Training Accuracy : 62.89%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 8 Training Loss 69957.89394504428\n",
            "Epoch 8 : Training Accuracy : 64.66%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 9 Training Loss 62529.69557535236\n",
            "Epoch 9 : Training Accuracy : 65.92%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 10 Training Loss 59116.44773751637\n",
            "Epoch 10 : Training Accuracy : 65.48%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 11 Training Loss 57832.35054400552\n",
            "Epoch 11 : Training Accuracy : 65.14%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 12 Training Loss 57523.37599014501\n",
            "Epoch 12 : Training Accuracy : 66.35%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 13 Training Loss 57366.51690856693\n",
            "Epoch 13 : Training Accuracy : 67.71%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 14 Training Loss 56949.218535528045\n",
            "Epoch 14 : Training Accuracy : 67.16%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 15 Training Loss 57191.21298096916\n",
            "Epoch 15 : Training Accuracy : 69.33%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 16 Training Loss 56804.02473210731\n",
            "Epoch 16 : Training Accuracy : 69.22%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 17 Training Loss 56931.57223203372\n",
            "Epoch 17 : Training Accuracy : 68.12%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 18 Training Loss 56989.654606187156\n",
            "Epoch 18 : Training Accuracy : 66.91%\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Epoch 19 Training Loss 57125.80712807186\n",
            "Epoch 19 : Training Accuracy : 64.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHQt_WWI6_9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def predict(pos):\n",
        "  v=model.forwardPass(np.array([X_test[pos]]))\n",
        "  plt.imshow(X_test[pos].reshape((28,28)))\n",
        "  v=v.T[0]\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "  l=max(v)\n",
        "  c=0\n",
        "  for i in v:\n",
        "      if i==l:\n",
        "          print(class_names[c])\n",
        "      c=c+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYzgo_nhKSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "eced5aca-b17e-48f6-80e3-3c551afbacc0"
      },
      "source": [
        "predict(44)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shirt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}